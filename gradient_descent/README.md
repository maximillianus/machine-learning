# Gradient Descent
Gradient descent is one of the optimization algorithm used to minimize cost function. To effectively use gradient descent, cost function must not be linear (otherwise gradient will always be a constant and cannot descend to 0).

There are 3 main gradient descent variation:
- Batch Gradient Descent (vanilla version)
- Stochastic Gradient Descent (computationally expensive but accurate)
- Mini batch Gradient Descent

Further reading below:
https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1
